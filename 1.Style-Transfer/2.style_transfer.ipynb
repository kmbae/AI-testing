{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5-eNE1_FUtO"
   },
   "source": [
    "# Neural Style Transfer\n",
    "Gaty et. al, Image Style Transfer Using Convolutional Neural Networks, CVPR 2016\n",
    "\n",
    "## Import Libraries\n",
    "필요한 라이브러리들을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1616056493771,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "TS-S935Nyf2R",
    "outputId": "38b2a3e2-c15c-4f61-b81a-17cff919f630"
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import time\n",
    "import os \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1616056494161,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "9OGK3ZFgNYDe",
    "outputId": "ac905f2e-901d-4372-94c0-0a5cd86741e2"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1162,
     "status": "ok",
     "timestamp": 1616056494162,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "3sOSv0HGcbSw",
    "outputId": "4e5f13d4-c0d9-4c85-d3c5-faf9d9607cad"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vovjzqY5FUtS"
   },
   "source": [
    "## Define Model\n",
    "ImageNet 데이터를 이용해 이미 학습된 VGG 19 네트워크를 이용합니다.\n",
    "VGG19 네트워크를 불러오기전에 먼저 네트워크를 정의합니다.\n",
    "\n",
    "그리고 각 레이어들에 이름을 설정해둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1156,
     "status": "ok",
     "timestamp": 1616056494162,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "BWfaMVfByf2W"
   },
   "outputs": [],
   "source": [
    "#vgg definition that conveniently let's you grab the outputs from any layer\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, pool='max'):\n",
    "        super(VGG, self).__init__()\n",
    "        #vgg modules\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        if pool == 'max':\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif pool == 'avg':\n",
    "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "    def forward(self, x, out_keys):\n",
    "        #style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "        #content_layers = ['r42']\n",
    "        out = {}\n",
    "        out['r11'] = F.relu(self.conv1_1(x))\n",
    "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
    "        out['p1'] = self.pool1(out['r12'])\n",
    "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
    "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
    "        out['p2'] = self.pool2(out['r22'])\n",
    "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
    "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
    "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
    "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
    "        out['p3'] = self.pool3(out['r34'])\n",
    "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
    "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
    "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
    "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
    "        out['p4'] = self.pool4(out['r44'])\n",
    "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
    "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
    "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
    "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
    "        out['p5'] = self.pool5(out['r54'])\n",
    "        return [out[key] for key in out_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiyI4JCpFUtV"
   },
   "source": [
    "## Define loss and Gram matrix\n",
    "\n",
    "아래 셀에서 Gram matrix와 loss를 구현해보세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1616056494162,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "lRXVt2OByf2Y"
   },
   "outputs": [],
   "source": [
    "# gram matrix and loss\n",
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self, x, y):        \n",
    "        ### Your code here ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################       \n",
    "        return out\n",
    "    \n",
    "    def GramMatrix(self, x):\n",
    "        ### Your code here ###\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9Dz2KyWFUtX"
   },
   "source": [
    "## Define parameters\n",
    "\n",
    "여러분이 원하는 파라미터를 조절하여 보세요\n",
    "\n",
    "1. `img_names`: style image로 사용할 이미지의 이름과 content image로 사용할 이미지의 이름을 정하는 변수\n",
    "    * 코드를 완성하고 자유롭게 이미지를 올려서 실험해보세요.\n",
    " \n",
    "    * ex) `['/path/to/style_image.jpg', '/path/to/content_image.jpg']`\n",
    "    \n",
    "\n",
    "2. `style_layers`: 어떤 layer의 feature와 style loss을 비교할지 정하는 변수\n",
    "    * VGG의 forward 부분을 참고해서 자유롭게 정해보세요.\n",
    "    \n",
    "    \n",
    "3. `content_layers`: 어떤 layer의 feature와 content loss를 비교할지 정하는 변수\n",
    "    * VGG의 forward 부분을 참고해서 자유롭게 정해보세요.\n",
    "    \n",
    "    \n",
    "4. `style_weights`: style_layers 변수에서 정한 layer에서의 weight\n",
    "    * style_layers와 길이가 같아야 합니다.\n",
    "    \n",
    "    \n",
    "5. `content_weights`: content_layers 변수에서 정한 layer에서의 weight\n",
    "    * content_layers와 길이가 같아야 합니다.\n",
    "    \n",
    "    \n",
    "6. `lr`: 나중에 optimizer의 초기 learning rate를 정해주는 변수\n",
    "    * 값의 크기에 따라 수렴이 빠를수도 느릴수도 있습니다. 자유롭게 바꾸면서 가장 빨리 수렴하는 값을 찾아보세요.\n",
    "    \n",
    "    \n",
    "7. `max_iter`: 최대 몇번까지 iteration 할지 정해주는 변수입니다.\n",
    "    * 너무 값이 커도 너무 작아도 결과가 이상할 수 있습니다. 적당한 값을 찾아보세요.\n",
    "\n",
    "\n",
    "8. `show_iter`: 변해가는 이미지를 확인하기 위해 얼마나 자주 결과를 출력할지 정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1147,
     "status": "ok",
     "timestamp": 1616056494163,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "x1HUcV0BFUtY"
   },
   "outputs": [],
   "source": [
    "### Your parameters here ###\n",
    "img_size = 512 \n",
    "\n",
    "style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "content_layers = ['r42']\n",
    "\n",
    "style_weights = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "#style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
    "content_weights = [1e-2]\n",
    "content_weights = [1e0]\n",
    "\n",
    "lr = 10\n",
    "\n",
    "max_iter = 500\n",
    "show_iter = 100\n",
    "#############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_-OEtQFFUtZ"
   },
   "source": [
    "## Define dataloader\n",
    "\n",
    "이미지를 불러오고 네트워크에 넣어주기 전 Tensor type으로 변환하고 RGB 값을 Normalize해 입력 형식을 맞춰줍니다.\n",
    "\n",
    "최종 결과의 이미지를 확인하기 위해 denormalize하는 모듈을 선언하여 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1616056494163,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "T7X8KjQPyf2a"
   },
   "outputs": [],
   "source": [
    "# pre and post processing for images\n",
    "\n",
    "prep = transforms.Compose([transforms.Resize(img_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul(255)),\n",
    "                          ])\n",
    "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul(1./255)),\n",
    "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
    "                           ])\n",
    "postpb = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "def postp(tensor): # to clip results in the range [0,1]\n",
    "    t = postpa(tensor)\n",
    "    t[t>1] = 1    \n",
    "    t[t<0] = 0\n",
    "    img = postpb(t)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHU-f2wyFUtc"
   },
   "source": [
    "## Load VGG\n",
    "\n",
    "VGGNet을 정의하고 ImageNet 데이터셋으로 학습된 weight의 checkpoint를 다운받고 GPU로 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3809,
     "status": "ok",
     "timestamp": 1616056496834,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "wcL48VLoyf2c",
    "outputId": "cb2fa4ba-7547-4b92-c777-4f62389f0428"
   },
   "outputs": [],
   "source": [
    "#get network\n",
    "vgg = VGG()\n",
    "\n",
    "while 1:\n",
    "    try:\n",
    "        vgg.load_state_dict(torch.load('vgg_conv.pth'))\n",
    "        print(\"Model loaded!!\")\n",
    "        break\n",
    "    except:\n",
    "        print(\"Cannot load model, downloading new model!!\")\n",
    "        os.system('rm vgg_conv.pth')\n",
    "        os.system('wget -c --no-check-certificate https://bethgelab.org/media/uploads/pytorch_models/vgg_conv.pth')\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "if torch.cuda.is_available():\n",
    "    vgg.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVrgt9DxFUtf"
   },
   "source": [
    "## Load image\n",
    "\n",
    "Style image와 content image를 읽어오고 Tensor 타입으로 변경합니다.\n",
    "\n",
    "만약 입력 이미지에 대한 경로가 정확하지 않을 경우 에러가 나면서 실행이 되지 않으니 입력 이미지의 이름을 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3492,
     "status": "ok",
     "timestamp": 1616057298371,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "_kdsQeg9yf2e",
    "outputId": "fcec228a-f966-4742-dc17-6b8176783ea7"
   },
   "outputs": [],
   "source": [
    "#load images, ordered as [style_image, content_image]\n",
    "\n",
    "style_image = './vangogh_starry_night.jpg'\n",
    "if not os.path.isfile(style_image):\n",
    "    os.system('wget https://raw.githubusercontent.com/leongatys/PytorchNeuralStyleTransfer/master/Images/vangogh_starry_night.jpg')\n",
    "    print(f'{style_image} download completed.')\n",
    "\n",
    "content_image = './Tuebingen_Neckarfront.jpg'\n",
    "if not os.path.isfile(content_image):\n",
    "    os.system('wget https://raw.githubusercontent.com/leongatys/PytorchNeuralStyleTransfer/master/Images/Tuebingen_Neckarfront.jpg')\n",
    "    print(f'{content_image} download completed.')\n",
    "\n",
    "\"\"\"\n",
    "content_image = './42553_50200_2730.JPG'\n",
    "if not os.path.isfile(content_image):\n",
    "    os.system('wget http://www.digitaltoday.co.kr/news/photo/201401/42553_50200_2730.JPG')\n",
    "\n",
    "content_image = './2071618_2066564_102.jpg'\n",
    "if not os.path.isfile(content_image): \n",
    "    os.system('wget https://cdn.gukjenews.com/news/photo/202009/2071618_2066564_102.jpg')\n",
    "\n",
    "content_image = 'pa89n23zvzl5y36t3j27.jpg'\n",
    "if not os.path.isfile(content_image):\n",
    "    os.system('wget https://img.insight.co.kr/static/2017/08/08/700/pa89n23zvzl5y36t3j27.jpg')\n",
    "    \n",
    "content_image = 'cfile26.uf.223952375358B8E10A9FCE.jpg'\n",
    "if not os.path.isfile(content_image):\n",
    "    os.system('wget https://insight-prd-data.s3.ap-northeast-2.amazonaws.com/wp-content/uploads/2/cfile26.uf.223952375358B8E10A9FCE.jpg')\n",
    "\"\"\"\n",
    "    \n",
    "img_names = [style_image, content_image]\n",
    "\n",
    "imgs = [Image.open(name) for i,name in enumerate(img_names)]\n",
    "print('Image loaded!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1616057299596,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "K2ekQpTWXr4G",
    "outputId": "54416485-34e3-4572-a1db-6e7c29718cf3"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEIIBKnuX3BU"
   },
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1616057300787,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "FOixw5fyXml-"
   },
   "outputs": [],
   "source": [
    "imgs_torch = [prep(img) for img in imgs]\n",
    "imgs_torch = [img.unsqueeze(0).to(device) for img in imgs_torch]\n",
    "style_image, content_image = imgs_torch\n",
    "\n",
    "opt_img = torch.randn(content_image.size()).type_as(content_image.data) #random init\n",
    "opt_img = content_image.data.clone() # Init as content image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1616057301402,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "8_iKBfeWPV13"
   },
   "outputs": [],
   "source": [
    "opt_img.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AAYx9E1FUti"
   },
   "source": [
    "## Show input images\n",
    "\n",
    "입력으로 사용된 Style image와 Content image를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 2074,
     "status": "ok",
     "timestamp": 1616057303737,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "qPo0x1t_yf2g",
    "outputId": "a469d501-5774-417e-afe1-c2e62df5aebc"
   },
   "outputs": [],
   "source": [
    "#display images\n",
    "for img in imgs:\n",
    "    imshow(img);show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iHvxOP7FUtk"
   },
   "source": [
    "## Define loss\n",
    "비교 기준이 되는 style feature와 content feature의 값을 구하고 loss를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1616057304099,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "YA1f1ZBbRwYg"
   },
   "outputs": [],
   "source": [
    "#define layers, loss functions, weights and compute optimization targets\n",
    "\n",
    "loss_layers = style_layers + content_layers\n",
    "loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "loss_fns = [loss_fn.to(device) for loss_fn in loss_fns]\n",
    "    \n",
    "#weights setting:\n",
    "weights = style_weights + content_weights\n",
    "\n",
    "#compute optimization targets\n",
    "style_targets = [A.detach() for A in vgg(style_image, style_layers)]\n",
    "content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
    "targets = style_targets + content_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iTCoWnaFUtm"
   },
   "source": [
    "## Training\n",
    "직접적으로 학습이 수행됩니다.\n",
    "\n",
    "Unlike training a network, we want to train the input image in order to minimise the content/style losses\n",
    "\n",
    "여러가지 optimizer들을 사용하고 parameter를 바꿔보면서 가장 좋은 결과를 만들어보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 72757,
     "status": "ok",
     "timestamp": 1616057378098,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "xU0NQaGVyf2j",
    "outputId": "d7d9b685-fdcf-4516-9ec2-4a07d7622b4d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.Adam([opt_img], lr)\n",
    "\n",
    "#run style transfer\n",
    "for n_iter in range(max_iter):\n",
    "    optimizer.zero_grad()\n",
    "    out = vgg(opt_img, loss_layers)\n",
    "    layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a, A in enumerate(out)]\n",
    "    loss = sum(layer_losses)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss and Show output image\n",
    "    if n_iter % show_iter == 0:\n",
    "        print('Iteration: {:03d}, loss: {}'.format(n_iter+1, loss.item()))\n",
    "        #display result\n",
    "        out_img = postp(opt_img.data[0].cpu().squeeze())\n",
    "        imshow(out_img)\n",
    "        show()\n",
    "        #gcf().set_size_inches(10,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_87g2M3BFUtp"
   },
   "source": [
    "## Show final result\n",
    "\n",
    "Style transfer의 최종 결과를 큰 사이즈로 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 1534,
     "status": "ok",
     "timestamp": 1616057383347,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "l49nkDd5yf2p",
    "outputId": "5c3c37d0-651a-4e26-abad-b477fb294190"
   },
   "outputs": [],
   "source": [
    "#display result\n",
    "print(\"Final Result!!\")\n",
    "out_img = postp(opt_img.data[0].cpu().squeeze())\n",
    "gcf().set_size_inches(10,10)\n",
    "imshow(out_img)\n",
    "show()\n",
    "\n",
    "# save the result image\n",
    "out_img.save('./result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86248,
     "status": "ok",
     "timestamp": 1616056579323,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "55I4t5i6ZtZr",
    "outputId": "9e936f3e-eccf-4474-b265-f041c486ecdb"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "style_transfer-answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
