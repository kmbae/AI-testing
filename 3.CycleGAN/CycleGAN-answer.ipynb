{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmJecJIpG0r3"
   },
   "source": [
    "# Image-to-Image translation (CycleGAN)\n",
    "Jun-Yan Zhu et. al, Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, ICCV 2017\n",
    "\n",
    "\n",
    "## Import Libraries\n",
    "필요한 라이브러리들을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wugzsIG_zOp2"
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9sbDRj6KG0r8"
   },
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ce-NEfsi5qgB"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, img_size=256):\n",
    "        self.root = root\n",
    "        if not os.path.exists(self.root):\n",
    "            raise Exception(\"[!] {} not exists.\".format(root))\n",
    "\n",
    "        self.name = os.path.basename(root)\n",
    "\n",
    "        self.paths = glob.glob(os.path.join(self.root, '*'))\n",
    "        if len(self.paths) == 0:\n",
    "            raise Exception(\"No images are found in {}\".format(self.root))\n",
    "        self.shape = list(Image.open(self.paths[0]).size) + [3]\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.paths[index]).convert('RGB')\n",
    "\n",
    "        return self.transform(image)*2 - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ema3ptThG0sA"
   },
   "source": [
    "## Define models\n",
    "### Define discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4gbQfTe9pKb"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, hidden_dims):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layers = []\n",
    "\n",
    "        prev_dim = hidden_dims[0]\n",
    "        self.layers.append(nn.Conv2d(input_channel, prev_dim, 4, 2, 1, bias=False))\n",
    "        self.layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        for out_dim in hidden_dims[1:]:\n",
    "            self.layers.append(nn.Conv2d(prev_dim, out_dim, 4, 2, 1, bias=False))\n",
    "            self.layers.append(nn.BatchNorm2d(out_dim))\n",
    "            self.layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            prev_dim = out_dim\n",
    "\n",
    "        self.layers.append(nn.Conv2d(prev_dim, output_channel, 4, 1, 0, bias=False))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.layer_module = nn.ModuleList(self.layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layer_module:\n",
    "            out = layer(out)\n",
    "        return out.view(out.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jr54yi5OG0sD"
   },
   "source": [
    "### Define generator\n",
    "\n",
    "The generator consists of encoding and decoding part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dg-Wy0G78SH"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, conv_dims, deconv_dims):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "\n",
    "        prev_dim = conv_dims[0]\n",
    "        self.encoder.append(nn.Conv2d(input_channel, prev_dim, 4, 2, 1, bias=False))\n",
    "        self.encoder.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        for out_dim in conv_dims[1:]:\n",
    "            self.encoder.append(nn.Conv2d(prev_dim, out_dim, 4, 2, 1, bias=False))\n",
    "            self.encoder.append(nn.BatchNorm2d(out_dim))\n",
    "            self.encoder.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            prev_dim = out_dim\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for out_dim in deconv_dims:\n",
    "            self.decoder.append(nn.ConvTranspose2d(prev_dim, out_dim, 4, 2, 1, bias=False))\n",
    "            self.decoder.append(nn.BatchNorm2d(out_dim))\n",
    "            self.decoder.append(nn.ReLU(True))\n",
    "            prev_dim = out_dim\n",
    "\n",
    "        self.decoder.append(nn.ConvTranspose2d(prev_dim, output_channel, 4, 2, 1, bias=False))\n",
    "        self.decoder.append(nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.encoder:\n",
    "            out = layer(out)\n",
    "        for layer in self.decoder:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xk5SGxCVG0sG"
   },
   "source": [
    "## Prepararation\n",
    "학습을 시작하기 위해 필요한 모든것들을 준비합니다.\n",
    "\n",
    "Parameter를 자유롭게 바꾸면서 학습시켜 보세요.\n",
    "\n",
    "주의!!!) CycleGAN 부터 학습 시간이 매우 길어집니다. 바로 결과가 나오지 않더라도 참고 기다리셔야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vYOk1CWzRFh"
   },
   "outputs": [],
   "source": [
    "### Your parameters ###\n",
    "dataset_dir = '/content/drive/My Drive/StyleTransfer/summer2winter_yosemite/'\n",
    "dataset_dir = '../datasets/summer2winter_yosemite/'\n",
    "\n",
    "# models\n",
    "conv_dims, deconv_dims = [64, 128, 256, 512], [256, 128, 64]\n",
    "\n",
    "# dataloader\n",
    "img_size = 64 #128\n",
    "batch_size = 64\n",
    "num_workers = 64*4\n",
    "\n",
    "# optimizer\n",
    "lr = 0.0001\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# training\n",
    "max_iter = 50000\n",
    "\n",
    "# log step\n",
    "log_disp_step = 100\n",
    "img_disp_step = 100\n",
    "net_save_step = 1000\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "path_snapshot = 'snapshots'\n",
    "path_outresult = 'results'\n",
    "\n",
    "### Your losses here ###\n",
    "#d = nn.MSELoss()\n",
    "L1 = nn.L1Loss()\n",
    "bce = nn.BCELoss()\n",
    "\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real & fake labels\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# prepare dataset\n",
    "dataset_A = Dataset(dataset_dir + 'trainA/', img_size=img_size)\n",
    "dataloader_A = torch.utils.data.DataLoader(dataset_A, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "dataset_B = Dataset(dataset_dir + 'trainB/', img_size=img_size)\n",
    "dataloader_B = torch.utils.data.DataLoader(dataset_B, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network and optimizer\n",
    "We define two discriminators $D_A, D_B$ and generators $G, F$.\n",
    "\n",
    "The discriminator $D_A$ predicts whether the input belongs to the domain $A$.\n",
    "\n",
    "The output of $D_A$ is given as:\n",
    "$$D_A(x)$$\n",
    "where $x\\in A$.\n",
    "\n",
    "The discriminator $D_B$ predicts whether the input belongs to the domain $B$.\n",
    "\n",
    "The output of $D_B$ is given as:\n",
    "$$D_B(x)$$\n",
    "where $x\\in B$.\n",
    "\n",
    "The generator $G$ translates the input from domain $A$ to domain $B$.\n",
    "\n",
    "The generator $F$ translates the input from domain $B$ to domain $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "G = Generator(3, 3, conv_dims, deconv_dims)\n",
    "F = Generator(3, 3, conv_dims, deconv_dims)\n",
    "\n",
    "D_A = Discriminator(3, 1, conv_dims)\n",
    "D_B = Discriminator(3, 1, conv_dims)\n",
    "\n",
    "G = torch.nn.DataParallel(G.cuda())\n",
    "F = torch.nn.DataParallel(F.cuda())\n",
    "\n",
    "D_A = torch.nn.DataParallel(D_A.cuda())\n",
    "D_B = torch.nn.DataParallel(D_B.cuda())\n",
    "\n",
    "# define optimizers\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "optimizer_G = optimizer(\n",
    "    chain(G.parameters(), F.parameters()),\n",
    "    lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "optimizer_D = optimizer(\n",
    "    chain(D_A.parameters(), D_B.parameters()),\n",
    "    lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYOdrPjpG0sK"
   },
   "source": [
    "# Training\n",
    "### Training step\n",
    "1. Generate fake images\n",
    "2. Calculate GAN loss for discriminator\n",
    "3. Update discriminator\n",
    "4. Calculate GAN loss for generator\n",
    "5. Calculate cycle consistency loss\n",
    "6. Update generator\n",
    "\n",
    "### Gnerating fake images\n",
    "Assume two elements $x_A, x_B$ sampled from different domains $A,B$:\n",
    "$$x_A\\in A,x_B\\in B$$\n",
    "\n",
    "The fake images $x_A', x_B'$ are generated using two generators $G,F$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "x_B'&=&G(x_A)\\\\\n",
    "x_A'&=&F(x_B)\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Cycle-consistency loss\n",
    "Two generators $G,F$ work as an inverse function to each other.\n",
    "\\begin{eqnarray}\n",
    "G&=&F^{-1}\\\\\n",
    "F&=&G^{-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The reconstructed images $\\hat{x}_A, \\hat{x}_B$ are given as:\n",
    "\\begin{eqnarray}\n",
    "\\hat{x}_A&=F(x_B')&=F(G(x_A))\\\\\n",
    "\\hat{x}_B&=G(x_A')&=G(F(x_B))\n",
    "\\end{eqnarray}\n",
    "\n",
    "The cycle-consistency loss $\\mathcal{L}_{cyc}$ is given as:\n",
    "$$\\mathcal{L}_{cyc}=||x_A-\\hat{x}_A||+||x_B-\\hat{x}_B||$$\n",
    "\n",
    "### Identity loss\n",
    "We wish generators to translate the input to another domain while preserving the core structures.\n",
    "\n",
    "The translated images $x_B', x_A'$ are given as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "x_B'&=&G(x_A)\\\\\n",
    "x_A'&=&F(x_B)\n",
    "\\end{eqnarray}\n",
    "\n",
    "while $x_A, x_B$ denotes the input images sampled from domain $A$ and $B$.\n",
    "\n",
    "The identity loss $\\mathcal{L}_{id}$ is given as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}_{id} = ||x_B'-x_A|| + ||x_A'-x_B||\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1WM-r2G2g6SNQRlaUqc0UW2jQvQ8fVwjx"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49115,
     "status": "ok",
     "timestamp": 1577157847476,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "fL8aES9K60pL",
    "outputId": "a234df3f-96b2-45e3-e5ed-ba09414cbf74"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(path_snapshot):\n",
    "    os.makedirs(path_snapshot)\n",
    "\n",
    "# Set training mode\n",
    "G.train()\n",
    "F.train()\n",
    "D_A.train()\n",
    "D_B.train()\n",
    "\n",
    "for i in range(max_iter):\n",
    "    try:\n",
    "        x_A = dataloader_A_iter.next().cuda()\n",
    "    except:\n",
    "        dataloader_A_iter = iter(dataloader_A)\n",
    "        x_A = dataloader_A_iter.next().cuda()\n",
    "    try:\n",
    "        x_B = dataloader_B_iter.next().cuda()\n",
    "    except:\n",
    "        dataloader_B_iter = iter(dataloader_B)\n",
    "        x_B = dataloader_B_iter.next().cuda()\n",
    "        \n",
    "    ## Update D_A & D_B\n",
    "    \n",
    "    optimizer_D.zero_grad()\n",
    "\n",
    "    ## Generate fake images\n",
    "    ### Your code here ###\n",
    "    \n",
    "    \n",
    "    x_B_fake = G(x_A)\n",
    "    x_A_fake = F(x_B)\n",
    "    \n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    ## Adversarial loss for two Discriminators\n",
    "    loss_D_A_real = bce(D_A(x_A), torch.ones_like(D_A(x_A)))\n",
    "    loss_D_A_fake = bce(D_A(x_A_fake.detach()), torch.zeros_like(D_A(x_A)))\n",
    "    \n",
    "    loss_D_B_real = bce(D_B(x_B), torch.ones_like(D_B(x_B)))\n",
    "    loss_D_B_fake = bce(D_B(x_B_fake.detach()), torch.zeros_like(D_B(x_B)))\n",
    "\n",
    "    loss_D = loss_D_A_real + loss_D_A_fake + loss_D_B_real + loss_D_B_fake\n",
    "    \n",
    "    loss_D.backward()\n",
    "\n",
    "    optimizer_D.step()\n",
    "\n",
    "    \n",
    "    ## Update G & F\n",
    "\n",
    "    optimizer_G.zero_grad()\n",
    "    \n",
    "    ## Adversarial loss for two Generators\n",
    "    # G\n",
    "    loss_G_B = bce(D_B(x_B_fake), torch.ones_like(D_B(x_B_fake)))\n",
    "    # F\n",
    "    loss_G_A = bce(D_A(x_A_fake), torch.ones_like(D_A(x_A_fake)))\n",
    "\n",
    "\n",
    "    ## Cycle-consistency loss\n",
    "    ### Your code here ###\n",
    "        \n",
    "    loss_G_cyc = L1(F(G(x_A)), x_A) + L1(G(F(x_B)), x_B)\n",
    "    \n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    ## Identity loss \n",
    "    ### Your code here ###\n",
    "    \n",
    "    loss_G_id = L1(G(x_A), x_A) + L1(F(x_B), x_B)\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    ## Total loss\n",
    "    loss_G = loss_G_A + loss_G_B + 10*loss_G_cyc + 0.5*loss_G_id\n",
    "\n",
    "    loss_G.backward()\n",
    "\n",
    "    optimizer_G.step()\n",
    "    \n",
    "    if (i + 1)%log_disp_step==0:\n",
    "        print(\"[{}] Iter {} / {}, D/loss: {}, G/loss: {}\".format(\n",
    "            str(datetime.now())[:-3], i + 1, max_iter,\n",
    "            loss_D.item(), loss_G.item()))\n",
    "\n",
    "    if (i + 1)%img_disp_step==0:\n",
    "        disp_img = (torch.cat([x_A[:8],x_B_fake[:8],x_B[:8],x_A_fake[:8]]) + 1)*0.5\n",
    "        disp_img = disp_img.cpu()\n",
    "        disp_img = torchvision.utils.make_grid(disp_img) \n",
    "        disp_img = transforms.ToPILImage()(disp_img)\n",
    "        disp_img.save('result_{:06d}.jpg'.format(i + 1))\n",
    "        gcf().set_size_inches(10,10)\n",
    "        imshow(disp_img)\n",
    "        show()\n",
    "        \n",
    "    if (i + 1)%net_save_step == 0:\n",
    "        torch.save(G.module.state_dict(), '{}/netG_{:06d}.pth'.format(path_snapshot, i + 1))\n",
    "        torch.save(F.module.state_dict(), '{}/netF_{:06d}.pth'.format(path_snapshot, i + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TF3jHxosG0sQ"
   },
   "outputs": [],
   "source": [
    "test_results = 'test_results'\n",
    "if not os.path.exists(test_results):\n",
    "    os.makedirs(test_results)\n",
    "\n",
    "# parameters\n",
    "g_load_dir = \"snapshots/netG_001000.pth\"\n",
    "f_load_dir = \"snapshots/netF_001000.pth\"\n",
    "\n",
    "# Set eval mode\n",
    "G.eval()\n",
    "F.eval()\n",
    "D_A.eval()\n",
    "D_B.eval()\n",
    "\n",
    "# prepare dataset\n",
    "dataset_A = Dataset(dataset_dir + 'testA/', img_size=img_size)\n",
    "dataloader_A = torch.utils.data.DataLoader(dataset_A, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "dataset_B = Dataset(dataset_dir + 'testB/', img_size=img_size)\n",
    "dataloader_B = torch.utils.data.DataLoader(dataset_B, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "\n",
    "# Load model\n",
    "G.module.load_state_dict(torch.load(g_load_dir))\n",
    "F.module.load_state_dict(torch.load(f_load_dir))\n",
    "\n",
    "dataloader_A_iter = iter(dataloader_A)\n",
    "dataloader_B_iter = iter(dataloader_B)\n",
    "\n",
    "i = 0\n",
    "while 1:\n",
    "    try:\n",
    "        x_A = dataloader_A_iter.next().cuda()\n",
    "        x_B = dataloader_B_iter.next().cuda()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        x_B_fake = G(x_A)\n",
    "        x_A_fake = F(x_B)\n",
    "        \n",
    "    #if (i + 1)%img_disp_step==0:\n",
    "    disp_img = (torch.cat([x_A,x_B_fake,x_B,x_A_fake]) + 1)*0.5\n",
    "    disp_img = disp_img.cpu()\n",
    "    disp_img = torchvision.utils.make_grid(disp_img, batch_size) \n",
    "    disp_img = transforms.ToPILImage()(disp_img)\n",
    "    disp_img.save('{}/test_result_{:06d}.jpg'.format(test_results, i + 1))\n",
    "    #gcf().set_size_inches(10,10)\n",
    "    #imshow(disp_img)\n",
    "        #show()\n",
    "    #if (i + 1)%test_log_step==0:\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN-answer-ywlee.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
