{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmJecJIpG0r3"
   },
   "source": [
    "# Image-to-Image translation (CycleGAN)\n",
    "Jun-Yan Zhu et. al, Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, ICCV 2017\n",
    "\n",
    "\n",
    "## Import Libraries\n",
    "필요한 라이브러리들을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wugzsIG_zOp2"
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import time\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9sbDRj6KG0r8"
   },
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ce-NEfsi5qgB"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, rootA, rootB, img_size=256, test=False):\n",
    "        self.rootA = rootA\n",
    "        self.rootB = rootB\n",
    "        if not os.path.exists(self.rootA):\n",
    "            raise Exception(\"[!] {} not exists.\".format(rootA))\n",
    "        if not os.path.exists(self.rootB):\n",
    "            raise Exception(\"[!] {} not exists.\".format(rootB))\n",
    "\n",
    "        self.name_A = os.path.basename(rootA)\n",
    "        self.name_B = os.path.basename(rootB)\n",
    "\n",
    "        self.paths_A = glob.glob(os.path.join(self.rootA, '*'))\n",
    "        if len(self.paths_A) == 0:\n",
    "            raise Exception(\"No images are found in {}\".format(self.rootA))\n",
    "            \n",
    "        self.paths_B = glob.glob(os.path.join(self.rootB, '*'))\n",
    "        if len(self.paths_B) == 0:\n",
    "            raise Exception(\"No images are found in {}\".format(self.rootB))\n",
    "        #self.shape = list(Image.open(self.paths_A[0]).size) + [3]\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        self.len = max(len(self.paths_A), len(self.paths_B))\n",
    "        self.lenA = len(self.paths_A)\n",
    "        self.lenB = len(self.paths_B)\n",
    "        \n",
    "        self.test = test\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_A = index % self.lenA \n",
    "        image_A = Image.open(self.paths_A[index_A]).convert('RGB')\n",
    "        \n",
    "        if self.test:\n",
    "            return {'x_A': self.transform(image_A)}\n",
    "        \n",
    "        #index_B = index % self.lenB\n",
    "        index_B = random.randint(0, self.lenB - 1)\n",
    "        image_B = Image.open(self.paths_B[index_B]).convert('RGB')\n",
    "\n",
    "        return {'x_A': self.transform(image_A), 'x_B': self.transform(image_B)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ema3ptThG0sA"
   },
   "source": [
    "## Define models\n",
    "### Define discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4gbQfTe9pKb"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, hidden_dims):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layers = []\n",
    "\n",
    "        prev_dim = hidden_dims[0]\n",
    "        self.layers.append(nn.Conv2d(input_channel, prev_dim, 4, 2, 1, bias=False))\n",
    "        self.layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        for out_dim in hidden_dims[1:]:\n",
    "            self.layers.append(nn.Conv2d(prev_dim, out_dim, 4, 2, 1, bias=False))\n",
    "            self.layers.append(nn.BatchNorm2d(out_dim))\n",
    "            self.layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            prev_dim = out_dim\n",
    "\n",
    "        self.layers.append(nn.Conv2d(prev_dim, output_channel, 4, 1, 0, bias=False))\n",
    "        #self.layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.layer_module = nn.ModuleList(self.layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layer_module:\n",
    "            out = layer(out)\n",
    "        return out.view(out.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jr54yi5OG0sD"
   },
   "source": [
    "### Define generator\n",
    "\n",
    "The generator consists of encoding and decoding part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dg-Wy0G78SH"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, conv_dims, deconv_dims):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "\n",
    "        prev_dim = conv_dims[0]\n",
    "        self.encoder.append(nn.Conv2d(input_channel, prev_dim, 4, 2, 1, bias=False))\n",
    "        self.encoder.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        for out_dim in conv_dims[1:]:\n",
    "            self.encoder.append(nn.Conv2d(prev_dim, out_dim, 4, 2, 1, bias=False))\n",
    "            self.encoder.append(nn.BatchNorm2d(out_dim))\n",
    "            self.encoder.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            prev_dim = out_dim\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for out_dim in deconv_dims:\n",
    "            self.decoder.append(nn.ConvTranspose2d(prev_dim, out_dim, 4, 2, 1, bias=False))\n",
    "            self.decoder.append(nn.BatchNorm2d(out_dim))\n",
    "            self.decoder.append(nn.ReLU(True))\n",
    "            prev_dim = out_dim\n",
    "\n",
    "        self.decoder.append(nn.ConvTranspose2d(prev_dim, output_channel, 4, 2, 1, bias=False))\n",
    "        self.decoder.append(nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.encoder:\n",
    "            out = layer(out)\n",
    "        for layer in self.decoder:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xk5SGxCVG0sG"
   },
   "source": [
    "## Prepararation\n",
    "학습을 시작하기 위해 필요한 모든것들을 준비합니다.\n",
    "\n",
    "Parameter를 자유롭게 바꾸면서 학습시켜 보세요.\n",
    "\n",
    "주의!!!) CycleGAN 부터 학습 시간이 매우 길어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vYOk1CWzRFh"
   },
   "outputs": [],
   "source": [
    "### Your parameters ###\n",
    "dataset_dir = '/content/drive/My Drive/StyleTransfer/summer2winter_yosemite/'\n",
    "dataset_dir = '../datasets/summer2winter_yosemite/'\n",
    "\n",
    "# models\n",
    "conv_dims, deconv_dims = [64, 128, 256, 512], [256, 128, 64]\n",
    "conv_dims, deconv_dims = [64, 128, 256], [128, 64]\n",
    "\n",
    "# dataloader\n",
    "img_size = 32 #128\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "\n",
    "# optimizer\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "#weight_decay = 0.0001\n",
    "\n",
    "# training\n",
    "max_epoch = 50000\n",
    "\n",
    "# log step\n",
    "log_disp_step = 100\n",
    "img_disp_step = 100\n",
    "net_save_step = 100\n",
    "\n",
    "# real & fake labels\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# loss weights\n",
    "lambda_cyc = 10\n",
    "lambda_idt = 5\n",
    "lambda_adv = 1\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "path_snapshot = 'snapshots'\n",
    "path_outresult = 'results'\n",
    "\n",
    "### Your losses here ###\n",
    "#d = nn.MSELoss()\n",
    "L1 = nn.L1Loss()\n",
    "adv = nn.MSELoss()# LSGAN\n",
    "#adv = nn.BCEWithLogitsLoss()# Original GAN\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "\"\"\"\n",
    "dataset_A = Dataset(dataset_dir + 'trainA/', img_size=img_size)\n",
    "dataloader_A = torch.utils.data.DataLoader(dataset_A, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "dataset_B = Dataset(dataset_dir + 'trainB/', img_size=img_size)\n",
    "dataloader_B = torch.utils.data.DataLoader(dataset_B, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\"\"\"\n",
    "\n",
    "# prepare dataset\n",
    "dataset = Dataset(dataset_dir + 'trainA/', dataset_dir + 'trainB/', img_size=img_size)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network and optimizer\n",
    "We define two discriminators $D_A, D_B$ and generators $G, F$.\n",
    "\n",
    "The discriminator $D_A$ predicts whether the input belongs to the domain $A$.\n",
    "\n",
    "The output of $D_A$ is given as:\n",
    "$$D_A(x)$$\n",
    "where $x\\sim A$.\n",
    "\n",
    "The discriminator $D_B$ predicts whether the input belongs to the domain $B$.\n",
    "\n",
    "The output of $D_B$ is given as:\n",
    "$$D_B(x)$$\n",
    "where $x\\sim B$.\n",
    "\n",
    "The generator $G$ translates the input from domain $A$ to domain $B$.\n",
    "\n",
    "The generator $F$ translates the input from domain $B$ to domain $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "G = Generator(3, 3, conv_dims, deconv_dims)\n",
    "F = Generator(3, 3, conv_dims, deconv_dims)\n",
    "\n",
    "D_A = Discriminator(3, 1, conv_dims)\n",
    "D_B = Discriminator(3, 1, conv_dims)\n",
    "\n",
    "G = torch.nn.DataParallel(G.cuda())\n",
    "F = torch.nn.DataParallel(F.cuda())\n",
    "\n",
    "D_A = torch.nn.DataParallel(D_A.cuda())\n",
    "D_B = torch.nn.DataParallel(D_B.cuda())\n",
    "\n",
    "# define optimizers\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "optimizer_G = optimizer(\n",
    "    chain(G.parameters(), F.parameters()),\n",
    "    lr=lr, betas=(beta1, beta2))#, weight_decay=weight_decay)\n",
    "optimizer_D = optimizer(\n",
    "    chain(D_A.parameters(), D_B.parameters()),\n",
    "    lr=lr, betas=(beta1, beta2))#, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYOdrPjpG0sK"
   },
   "source": [
    "## Training\n",
    "### Training step\n",
    "1. Generate fake images\n",
    "2. Calculate GAN loss for discriminator\n",
    "3. Update discriminator\n",
    "4. Calculate GAN loss for generator\n",
    "5. Calculate cycle consistency loss\n",
    "6. Update generator\n",
    "\n",
    "### Adversarial loss (Generating fake images)\n",
    "Assume two elements $x_A, x_B$ sampled from different domains $A,B$:\n",
    "$$x_A\\sim A,x_B\\sim B$$\n",
    "\n",
    "The fake images $x_A', x_B'$ are generated using two generators $G,F$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "x_B'&=&G(x_A)\\\\\n",
    "x_A'&=&F(x_B)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Therefore adversarial loss $\\mathcal{L}_{adv}$ is given as:\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}_{adv} &= &\\mathbb{E}_{x_A}[\\log(D_A(x_A))] + \\mathbb{E}_{x_B}[\\log(1-D_A(F(x_B))) ]\\\\\n",
    "&&+ \\mathbb{E}_{x_B}[\\log(D_B(x_B))] + \\mathbb{E}_{x_A}[\\log(1-D_B(G(x_A))]\n",
    "\\end{eqnarray}\n",
    "\n",
    "For LSGAN, the adversarial loss $\\mathcal{L}_{adv}$ is given as:\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}_{adv} &= &\\mathbb{E}_{x_A}[||(D_A(x_A))-1||^2] + \\mathbb{E}_{x_B}[||D_A(F(x_B)))||^2 ]\\\\\n",
    "&&+ \\mathbb{E}_{x_B}[||D_B(x_B))-1||^2] + \\mathbb{E}_{x_A}[||D_B(G(x_A))||^2]\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Cycle-consistency loss\n",
    "Two generators $G,F$ work as an inverse function of each other.\n",
    "\\begin{eqnarray}\n",
    "G&=&F^{-1}\\\\\n",
    "F&=&G^{-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The reconstructed images $\\hat{x}_A, \\hat{x}_B$ are given as:\n",
    "\\begin{eqnarray}\n",
    "\\hat{x}_A&=F(x_B')&=F(G(x_A))\\\\\n",
    "\\hat{x}_B&=G(x_A')&=G(F(x_B))\n",
    "\\end{eqnarray}\n",
    "\n",
    "The cycle-consistency loss $\\mathcal{L}_{cyc}$ is given as:\n",
    "$$\\mathcal{L}_{cyc}=\\mathbb{E}_{x_A}[||x_A-\\hat{x}_A||]+\\mathbb{E}_{x_B}[||x_B-\\hat{x}_B||]$$\n",
    "\n",
    "### Identity loss\n",
    "We wish generators to translate the input to another domain while preserving the core structures.\n",
    "\n",
    "The translated images $x_B', x_A'$ are given as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "x_B'&=&G(x_A)\\\\\n",
    "x_A'&=&F(x_B)\n",
    "\\end{eqnarray}\n",
    "\n",
    "while $x_A, x_B$ denotes the input images sampled from domain $A$ and $B$.\n",
    "\n",
    "The identity loss $\\mathcal{L}_{idt}$ is given as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}_{idt} = \\mathbb{E}_{x_A}[||x_B'-x_A||] + \\mathbb{E}_{x_B}[||x_A'-x_B||]\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Total loss\n",
    "The total loss $\\mathcal{L}$ is given as:\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L} = \\mathcal{L}_{adv} + \\lambda_{cyc}\\mathcal{L}_{cyc} + \\lambda_{idt}\\mathcal{L}_{idt}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\lambda_{cyc}$ and $\\lambda_{idt}$ are weights.\n",
    "\n",
    "The final objective is to find optimal $G^*$, $F^*$, $D_A^*$, and $D_B^*$, which are given as:\n",
    "\\begin{eqnarray}\n",
    "G^*, F^*, D_A^*, D_B^* = \\arg\\min_{G,F}\\max_{D_A, D_B}\\mathcal{L} \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1WM-r2G2g6SNQRlaUqc0UW2jQvQ8fVwjx"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49115,
     "status": "ok",
     "timestamp": 1577157847476,
     "user": {
      "displayName": "Kangmin Bae",
      "photoUrl": "",
      "userId": "16142197103916175299"
     },
     "user_tz": -540
    },
    "id": "fL8aES9K60pL",
    "outputId": "a234df3f-96b2-45e3-e5ed-ba09414cbf74"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(path_snapshot):\n",
    "    os.makedirs(path_snapshot)\n",
    "\n",
    "# Set training mode\n",
    "G.train()\n",
    "F.train()\n",
    "D_A.train()\n",
    "D_B.train()\n",
    "\n",
    "\n",
    "\n",
    "pbar = tqdm.tqdm(range(max_epoch), dynamic_ncols=True)\n",
    "for i in pbar:\n",
    "    \n",
    "\n",
    "    for data in dataloader:\n",
    "\n",
    "        x_A = data['x_A'].cuda()\n",
    "        x_B = data['x_B'].cuda()\n",
    "\n",
    "        ## Update D_A & D_B\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        ## Generate fake images\n",
    "        ### Your code here ###\n",
    "\n",
    "\n",
    "        x_B_fake = G(x_A)\n",
    "        x_A_fake = F(x_B)\n",
    "\n",
    "\n",
    "        #######################\n",
    "\n",
    "        ## Adversarial loss for two Discriminators\n",
    "        loss_D_A_real = adv(D_A(x_A), torch.ones_like(D_A(x_A)))\n",
    "        loss_D_A_fake = adv(D_A(x_A_fake.detach()), torch.zeros_like(D_A(x_A)))\n",
    "\n",
    "        loss_D_B_real = adv(D_B(x_B), torch.ones_like(D_B(x_B)))\n",
    "        loss_D_B_fake = adv(D_B(x_B_fake.detach()), torch.zeros_like(D_B(x_B)))\n",
    "\n",
    "        loss_D = 0.5*(loss_D_A_real + loss_D_A_fake + loss_D_B_real + loss_D_B_fake)\n",
    "\n",
    "        loss_D.backward()\n",
    "\n",
    "        optimizer_D.step()\n",
    "\n",
    "\n",
    "        ## Update G & F\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        ## Adversarial loss for two Generators\n",
    "        # G\n",
    "        loss_G_B = adv(D_B(x_B_fake), torch.ones_like(D_B(x_B_fake)))\n",
    "        # F\n",
    "        loss_G_A = adv(D_A(x_A_fake), torch.ones_like(D_A(x_A_fake)))\n",
    "        loss_G_adv = 0.5*(loss_G_A + loss_G_B)\n",
    "\n",
    "\n",
    "        ## Cycle-consistency loss\n",
    "        ### Your code here ###\n",
    "\n",
    "        loss_G_cyc = L1(F(x_B_fake), x_A) + L1(G(x_A_fake), x_B)\n",
    "        loss_G_cyc *= lambda_cyc\n",
    "\n",
    "        #######################\n",
    "\n",
    "        ## Identity loss \n",
    "        ### Your code here ###\n",
    "\n",
    "        loss_G_idt = L1(G(x_A), x_A) + L1(F(x_B), x_B)\n",
    "        loss_G_idt *= lambda_idt\n",
    "\n",
    "        #######################\n",
    "\n",
    "        ## Total loss\n",
    "        loss_G = loss_G_adv + loss_G_cyc + loss_G_idt\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "        \n",
    "    if (i + 1)%log_disp_step==0:\n",
    "        print(\"[{}] Epoch {} / {}, D/loss: {:.4f}, G/loss: {:.4f}, G_adv/loss: {:.4f}, G_cyc/loss: {:.4f}, G_idt/loss: {:.4f}\".format(\n",
    "            str(datetime.now())[:-3], i + 1, max_epoch,\n",
    "            loss_D.item(), loss_G.item(), loss_G_adv.item(), loss_G_cyc.item(), loss_G_idt.item()))\n",
    "\n",
    "    if (i + 1)%img_disp_step==0:\n",
    "        disp_img = (torch.cat([x_A[:8],x_B_fake[:8],x_B[:8],x_A_fake[:8]]) + 1)*0.5\n",
    "        disp_img = disp_img.cpu()\n",
    "        disp_img = torchvision.utils.make_grid(disp_img) \n",
    "        disp_img = transforms.ToPILImage()(disp_img)\n",
    "        #disp_img.save('result_{:06d}.jpg'.format(i + 1))\n",
    "        gcf().set_size_inches(10,10)\n",
    "        imshow(disp_img)\n",
    "        show()\n",
    "\n",
    "    if (i + 1)%net_save_step == 0:\n",
    "        torch.save(G.module.state_dict(), '{}/netG_{:06d}.pth'.format(path_snapshot, i + 1))\n",
    "        torch.save(F.module.state_dict(), '{}/netF_{:06d}.pth'.format(path_snapshot, i + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "### Parameters\n",
    "\n",
    "- ```batch_size```: the number of batch size while loading input image from dataset (bigger than 1 is not supported)\n",
    "- ```max_test_images```: the maximum number of images to test from domain $A$ and $B$\n",
    "- ```g_load_dir```: path to the checkpoint file of generator $G$\n",
    "- ```f_load_dir```: path to the checkpoint file of generator $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TF3jHxosG0sQ"
   },
   "outputs": [],
   "source": [
    "test_results = 'test_results'\n",
    "if not os.path.exists(test_results):\n",
    "    os.makedirs(test_results)\n",
    "\n",
    "# parameters\n",
    "batch_size = 1\n",
    "max_test_images = 3\n",
    "\n",
    "g_load_dir = \"snapshots/netG_002700.pth\"\n",
    "f_load_dir = \"snapshots/netF_002700.pth\"\n",
    "\n",
    "# Set eval mode\n",
    "G.eval()\n",
    "F.eval()\n",
    "D_A.eval()\n",
    "D_B.eval()\n",
    "\n",
    "# Load model\n",
    "G.module.load_state_dict(torch.load(g_load_dir))\n",
    "F.module.load_state_dict(torch.load(f_load_dir))\n",
    "\n",
    "\n",
    "# prepare dataset\n",
    "dataset = Dataset(dataset_dir + 'testA/', dataset_dir + 'testB/', img_size=img_size, test=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           shuffle=False,\n",
    "                                           drop_last=False,\n",
    "                                        )\n",
    "\n",
    "print('Results of x_A to domain B')\n",
    "\n",
    "for idx, data in enumerate(dataloader):\n",
    "    if idx>max_test_images:\n",
    "        break\n",
    "    x_A = data['x_A'].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_B_fake = G(x_A)\n",
    "\n",
    "    disp_img = (torch.cat([x_A,x_B_fake]) + 1)*0.5\n",
    "    disp_img = disp_img.cpu()\n",
    "    disp_img = torchvision.utils.make_grid(disp_img) \n",
    "    disp_img = transforms.ToPILImage()(disp_img)\n",
    "    #disp_img.save('{}/test_result_{:06d}.jpg'.format(test_results, i + 1))\n",
    "    gcf().set_size_inches(5, 5)\n",
    "    imshow(disp_img)\n",
    "    show() \n",
    "    \n",
    "    \n",
    "\n",
    "# prepare dataset\n",
    "dataset = Dataset(dataset_dir + 'testB/', dataset_dir + 'testA/', img_size=img_size, test=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           num_workers=num_workers, \n",
    "                                           shuffle=False,\n",
    "                                           drop_last=False,\n",
    "                                        )\n",
    "\n",
    "print('Results of x_B to domain A')\n",
    "for idx, data in enumerate(dataloader):\n",
    "    if idx>max_test_images:\n",
    "        break\n",
    "    x_A = data['x_A'].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_B_fake = G(x_A)\n",
    "\n",
    "    disp_img = (torch.cat([x_A,x_B_fake]) + 1)*0.5\n",
    "    disp_img = disp_img.cpu()\n",
    "    disp_img = torchvision.utils.make_grid(disp_img) \n",
    "    disp_img = transforms.ToPILImage()(disp_img)\n",
    "    #disp_img.save('{}/test_result_{:06d}.jpg'.format(test_results, i + 1))\n",
    "    gcf().set_size_inches(5,5)\n",
    "    imshow(disp_img)\n",
    "    show() \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN-answer-ywlee.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
