{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v5vQsRwFTBc"
   },
   "source": [
    "# Neural Style Transfer (2)\n",
    "Xun Huang et. al, \"Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,\" ICCV 2017\n",
    "\n",
    "\n",
    "## Import Libraries\n",
    "필요한 라이브러리들을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhSSN0fuFTBg",
    "outputId": "b21d3418-82f7-4e16-8713-222ff437b052"
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "# Mount Google drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "#import visdom\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "#import ipdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import utils, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KR5QNV0tFTBh"
   },
   "source": [
    "## Custom dataset\n",
    "PyTorch에서 custom dataset을 만들기 위해서는 다음의 3가지가 꼭 있어야합니다.\n",
    "1. `__init__`\n",
    "    - 데이터셋을 불러올때 필요한 변수들을 초기화하고 저장하는 역할\n",
    "    - ex) 데이터셋의 위치 정보, 데이터셋에 존재하는 모든 파일 이름, Image 형식에서 Tensor 형식으로 변환 방법 등\n",
    "2. `__len__`\n",
    "    - 데이터셋의 총 길이를 반화하는 함수\n",
    "    - return과 함께 데이터셋에 존재하는 모든 데이터의 수를 반환하여야합니다.\n",
    "3. `__getitem__`\n",
    "    - 데이터셋에서 직접적으로 데이터를 가져오는 함수\n",
    "    - return과 함께 전처리가 완료된 데이터를 반환하여야합니다.\n",
    "    - return한 데이터는 데이터 타임이 Tensor인 것을 권장하고 모든 데이터에 대해 shape이 같아야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHIoPmnzFTBh"
   },
   "outputs": [],
   "source": [
    "class DataManager(Dataset):\n",
    "    def __init__(self, path_content, path_style, random_crop=True):\n",
    "        self.path_content = path_content\n",
    "        self.path_style = path_style\n",
    "\n",
    "        # Preprocessing for imagenet pre-trained network\n",
    "        if random_crop:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomCrop((256, 256)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.CenterCrop((256,256)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Convert pre-processed images to original images\n",
    "        self.restore = transforms.Compose(\n",
    "            [\n",
    "                transforms.Normalize(mean=[-2.118, -2.036, -1.804],\n",
    "                                     std=[4.367, 4.464, 4.444]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.list_content = listdir(self.path_content)\n",
    "        self.list_style = listdir(self.path_style)\n",
    "\n",
    "        self.num_content = len(self.list_content)\n",
    "        self.num_style = len(self.list_style)\n",
    "\n",
    "        assert self.num_content > 0\n",
    "        assert self.num_style > 0\n",
    "\n",
    "        self.num = min(self.num_content, self.num_style)\n",
    "\n",
    "        print('Content root : %s' % (self.path_content))\n",
    "        print('Style root : %s' % (self.path_style))\n",
    "        print('Number of content images : %d' % (self.num_content))\n",
    "        print('Number of style images : %d' % (self.num_style))\n",
    "        print('Dataset size : %d' % (self.num))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_con = self.path_content + '/' + self.list_content[idx]\n",
    "        path_to_sty = self.path_style + '/' + self.list_style[idx]\n",
    "\n",
    "        img_con = Image.open(path_to_con)\n",
    "        img_con = self.transform(img_con)\n",
    "\n",
    "        img_sty = Image.open(path_to_sty)\n",
    "        img_sty = self.transform(img_sty)\n",
    "\n",
    "        sample = {'content': img_con, 'style': img_sty}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbhprUT2FTBh"
   },
   "source": [
    "## Define model\n",
    "\n",
    "Encoder와 Decoder의 모델을 정의해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnE3ag1pFTBi"
   },
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(512, 256, (3, 3)),\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(256, 256, (3, 3)),\n",
    "        nn.ReLU(),\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(256, 256, (3, 3)),\n",
    "        nn.ReLU(),\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(256, 256, (3, 3)),\n",
    "        nn.ReLU(),\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(256, 128, (3, 3)),\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(128, 128, (3, 3)),\n",
    "        nn.ReLU(),\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(128, 64, (3, 3)),\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(64, 64, (3, 3)),\n",
    "        nn.ReLU(),\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(64, 3, (3, 3))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEvm-wGeFTBi"
   },
   "source": [
    "### AdaIN Layer & Loss function\n",
    "다음의 함수들을 작성하여 보세요.\n",
    "1. `AdaINLayer`\n",
    "    - `AdaINLayer`는 `calc_mean_std` 함수를 이용해 $x$와 $y$의 평균과 표준편차를 얻습니다.\n",
    "    - `AdaINLayer`는 다음의 식과 같이 구현할 수 있습니다.\n",
    "$$\\text{AdaIN}(x,y)=\\sigma(y)\\big(\\frac{x-\\mu(x)}{\\sigma(x)}\\big)+\\mu(y)$$\n",
    "\n",
    "2. `calc_mean_std`\n",
    "    - `calc_mean_std`는 $x$와 $y$의 평균과 표준편차를 계산합니다.\n",
    "\n",
    "3. `calc_loss`\n",
    "    - `calc_loss`는 content loss 또는 style loss에 따라 다른 방법으로 계산합니다.\n",
    "    - Content loss의 계산 방법은 다음과 같습니다.\n",
    "    $$\\mathcal{L}_{con}(x,y)=MSE(x,y)$$\n",
    "    - Style loss의 계산 방법은 다음과 같습니다.\n",
    "    $$\\mathcal{L}_{con}(x,y)=MSE(\\mu(x)-\\mu(y))+MSE(\\sigma(x)-\\sigma(y))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZG4rRbyFTBi"
   },
   "outputs": [],
   "source": [
    "class StyleTransferNet(nn.Module):\n",
    "    def __init__(self, w_style=0.1, alpha=1):\n",
    "        super(StyleTransferNet, self).__init__()\n",
    "\n",
    "        self.w_style = w_style\n",
    "        self.alpha = alpha\n",
    "        encoder = list(models.vgg19(pretrained=True).features.children())\n",
    "        self.enc_1 = nn.Sequential(*encoder[:2])  # input -> relu1_1\n",
    "        self.enc_2 = nn.Sequential(*encoder[2:7])  # relu1_1 -> relu2_1\n",
    "        self.enc_3 = nn.Sequential(*encoder[7:12])  # relu2_1 -> relu3_1\n",
    "        self.enc_4 = nn.Sequential(*encoder[12:21])  # relu3_1 -> relu4_1\n",
    "        self.decoder = decoder\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        for param in self.enc_1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.enc_2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.enc_3.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.enc_4.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def AdaINLayer(self, x, y):\n",
    "        Bx, Cx, Hx, Wx = x.shape\n",
    "        By, Cy, Hy, Wy = y.shape\n",
    "\n",
    "        assert Bx == By\n",
    "        assert Cx == Cy\n",
    "        \n",
    "        ### Your code here ###\n",
    "        \n",
    "        \n",
    "        style_mu, style_sigma = self.calc_mean_std(y)\n",
    "        content_mu, content_sigma = self.calc_mean_std(x)\n",
    "        \n",
    "        normalized_feat = (x - content_mu) / content_sigma\n",
    "        output = normalized_feat * style_sigma + style_mu\n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        return output\n",
    "\n",
    "    def encode_with_intermediate(self, x):\n",
    "        results = [x]\n",
    "        h = self.enc_1(x)\n",
    "        results.append(h)\n",
    "        h = self.enc_2(h)\n",
    "        results.append(h)\n",
    "        h = self.enc_3(h)\n",
    "        results.append(h)\n",
    "        h = self.enc_4(h)\n",
    "        results.append(h)\n",
    "        return results[1:]\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc_1(x)\n",
    "        h = self.enc_2(h)\n",
    "        h = self.enc_3(h)\n",
    "        h = self.enc_4(h)\n",
    "        return h\n",
    "\n",
    "    def calc_mean_std(self, feat, eps=1e-5):\n",
    "        # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "        size = feat.size()\n",
    "        assert (len(size) == 4)\n",
    "        N, C = size[:2]\n",
    "        \n",
    "        \n",
    "        ### Your code here ###\n",
    "                \n",
    "        feat_mean = torch.mean(feat, [2,3], keepdim=True)\n",
    "        feat_std = torch.std(feat, [2,3], keepdim=True) + eps\n",
    "        #feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "        #feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "        #feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "        \n",
    "        ######################\n",
    "        return feat_mean, feat_std\n",
    "\n",
    "    def calc_loss(self, x, y, loss_type):\n",
    "        assert (x.size() == y.size())\n",
    "        assert (y.requires_grad is False)\n",
    "        assert loss_type=='content' or loss_type=='style'\n",
    "        if loss_type=='content':\n",
    "            ### Your code here ###\n",
    "            \n",
    "            \n",
    "            out = self.mse_loss(x, y)\n",
    "                    \n",
    "            \n",
    "            ######################\n",
    "            return out\n",
    "        else:\n",
    "            ### Your code here ###\n",
    "            \n",
    "            \n",
    "            input_mean, input_std = self.calc_mean_std(x)\n",
    "            target_mean, target_std = self.calc_mean_std(y)\n",
    "            out = self.mse_loss(input_mean, target_mean) + \\\n",
    "                     self.mse_loss(input_std, target_std)\n",
    "            \n",
    "            \n",
    "            ######################\n",
    "            return out\n",
    "        \n",
    "    def infer_alpha(self, t, content_feat):\n",
    "        img_result = []\n",
    "        alpha = 0.\n",
    "        a = alpha * t + (1 - alpha) * content_feat\n",
    "        img_result.append(self.decoder(a))\n",
    "        alpha = 0.25\n",
    "        a = alpha * t + (1 - alpha) * content_feat\n",
    "        img_result.append(self.decoder(a))\n",
    "        alpha = 0.5\n",
    "        a = alpha * t + (1 - alpha) * content_feat\n",
    "        img_result.append(self.decoder(a))\n",
    "        alpha = 0.75\n",
    "        a = alpha * t + (1 - alpha) * content_feat\n",
    "        img_result.append(self.decoder(a))\n",
    "        alpha = 1.\n",
    "        a = alpha * t + (1 - alpha) * content_feat\n",
    "        img_result.append(self.decoder(a))\n",
    "\n",
    "        return img_result\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        style_feats = self.encode_with_intermediate(y)\n",
    "        content_feat = self.encode(x)\n",
    "        t = self.AdaINLayer(content_feat, style_feats[-1])\n",
    "        if not self.training:\n",
    "            return self.infer_alpha(t, content_feat)\n",
    "\n",
    "        img_result = self.decoder(t)\n",
    "        g_t_feats = self.encode_with_intermediate(img_result)\n",
    "\n",
    "        loss_c = self.calc_loss(g_t_feats[-1], t, 'content')\n",
    "        loss_s = self.calc_loss(g_t_feats[0], style_feats[0], 'style')\n",
    "        for i in range(1, 4):\n",
    "            loss_s += self.calc_loss(g_t_feats[i], style_feats[i], 'style')\n",
    "\n",
    "        loss = loss_c + self.w_style*loss_s\n",
    "\n",
    "        return loss, img_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQc5vz5DFTBk"
   },
   "source": [
    "## Parameters\n",
    "optimizer와 weights 부분을 바꾸면서 최적의 학습 파라미터를 찾아보세요.\n",
    "\n",
    "만약 CUDA_MEMORY_ERROR와 같은 에러가 발생한다면 dataloader에 있는 `batch_size`를 줄여보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIpx1pdbFTBm"
   },
   "outputs": [],
   "source": [
    "### Your parameters here ###\n",
    "# path\n",
    "path_snapshot = 'snapshots'\n",
    "path_content = '/content/drive/MyDrive/Classroom/StyleTransfer/dataset/train/content'\n",
    "path_style = '/content/drive/MyDrive/Classroom/StyleTransfer/dataset/train/style'\n",
    "\n",
    "path_content = 'dataset/train/content'\n",
    "path_style = 'dataset/train/style'\n",
    "\n",
    "path_content = '../datasets/dataset/train/content'\n",
    "path_style = '../datasets/dataset/train/style'\n",
    "\n",
    "# dataloader\n",
    "batch_size = 16\n",
    "num_epoch = 600\n",
    "num_workers = 16\n",
    "\n",
    "# optimizer\n",
    "lr_init = 0.0001#0.001\n",
    "lr_decay_step = num_epoch/2\n",
    "momentum = 0.9\n",
    "weight_decay = 1.0e-5\n",
    "\n",
    "# weights\n",
    "w_style = 10\n",
    "alpha = 1\n",
    "\n",
    "# log steps\n",
    "log_disp_step = 100\n",
    "img_disp_step = 10\n",
    "\n",
    "\n",
    "#############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr3LIOeVFTBm"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K4RmRVz2FTBm",
    "outputId": "ba16cb6d-2827-43d7-be1a-16918d204cf2"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(path_snapshot):\n",
    "    os.makedirs(path_snapshot)\n",
    "    \n",
    "# Data loader\n",
    "dm = DataManager(path_content, path_style, random_crop=True)\n",
    "dl = DataLoader(dm, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "num_train = dm.num\n",
    "num_batch = np.ceil(num_train / batch_size)\n",
    "loss_train_avg = np.zeros(num_epoch)\n",
    "\n",
    "net = StyleTransferNet(w_style, alpha)\n",
    "\n",
    "# Define your optimizer in here to train only the decoder network\n",
    "optimizer = torch.optim.Adam(net.decoder.parameters(), lr=lr_init)\n",
    "\n",
    "# Multi-GPU if available\n",
    "net = nn.DataParallel(net.cuda(), device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "# Start training\n",
    "for epoch in range(0, num_epoch):\n",
    "    net.train()\n",
    "    running_loss_train = 0\n",
    "    np.random.shuffle(dl.dataset.list_style)\n",
    "\n",
    "    for i, data in enumerate(dl, 0):\n",
    "        img_con = data['content'].cuda()\n",
    "        img_sty = data['style'].cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, img_result = net(img_con, img_sty)\n",
    "\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss_train += loss\n",
    "\n",
    "        if (i+1)%log_disp_step==0:\n",
    "            print('[%s] Epoch %3d / %3d, Batch %5d / %5d, Loss = %12.8f' %\n",
    "              (str(datetime.now())[:-3], epoch + 1, num_epoch,\n",
    "               i + 1, num_batch, loss))\n",
    "\n",
    "    loss_train_avg[epoch] = running_loss_train / num_batch\n",
    "\n",
    "    print('[%s] Epoch %3d / %3d, Avg Loss = %12.8f' % \\\n",
    "          (str(datetime.now())[:-3], epoch + 1, num_epoch,\n",
    "           loss_train_avg[epoch]))\n",
    "\n",
    "    if (epoch + 1)%img_disp_step == 0:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            img_result = net(img_con, img_sty)\n",
    "            img_result.insert(0, img_con)\n",
    "            img_result.append(img_sty)\n",
    "            img_cat = torch.cat(img_result, dim=3)\n",
    "            img_cat = torch.unbind(img_cat, dim=0)\n",
    "            img_cat = torch.cat(img_cat, dim=1)\n",
    "            img_cat = dm.restore(img_cat.data.cpu())\n",
    "            output_img = torch.clamp(img_cat, 0, 1)\n",
    "            \n",
    "        tt = transforms.ToPILImage()(output_img)\n",
    "        gcf().set_size_inches(10,10)\n",
    "        imshow(tt)\n",
    "        show()\n",
    "\n",
    "    # Snapshot\n",
    "    if ((epoch + 1) % 100) == 0:\n",
    "        torch.save(net.module.state_dict(), '%s/epoch_%06d.pth' % (path_snapshot, epoch + 1))\n",
    "\n",
    "#torch.save(net.state_dict(), '%s/epoch_%06d.pth' % (path_snapshot, epoch + 1))\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcgxeE2oFTBn"
   },
   "source": [
    "## Testing\n",
    "\n",
    "테스트를 수행하기 위한 코드로 테스트 데이터셋에 대한 결과가 colab에 저장됩니다.\n",
    "\n",
    "`model_path`에 학습한 checkpoint 파일의 경로를 입력하여 학습한 모델을 테스트해보세요.\n",
    "\n",
    "주의!!!) checkpoint 파일과 테스트 결과는 colab를 초기화 할 경우 모두 사라집니다. 꼭 본인의 pc로 백업을 받아 두세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz7Cq2VQFTBn"
   },
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "path_content = '/content/drive/MyDrive/Classroom/StyleTransfer/dataset/test/content'\n",
    "path_style = '/content/drive/MyDrive/Classroom/StyleTransfer/dataset/test/style'\n",
    "\n",
    "path_content = 'dataset/test/content'\n",
    "path_style = 'dataset/test/style'\n",
    "\n",
    "### Your parameters here ###\n",
    "model_path = 'snapshots/epoch_000100.pth'\n",
    "\n",
    "############################\n",
    "\n",
    "batch_size = 1\n",
    "w_style = 10\n",
    "alpha = 1\n",
    "disp_step = 1\n",
    "\n",
    "# Data loader\n",
    "dm = DataManager(path_content, path_style, random_crop=False)\n",
    "dl = DataLoader(dm, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "\n",
    "net = StyleTransferNet(w_style, alpha)\n",
    "net = nn.DataParallel(net.cuda(), device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "# Load model\n",
    "state_dict = torch.load(model_path)\n",
    "net.module.load_state_dict(state_dict)\n",
    "\n",
    "# Start testing\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dl, 0):\n",
    "        img_con = data['content'].cuda()\n",
    "        img_sty = data['style'].cuda()\n",
    "\n",
    "        img_result = net(img_con, img_sty)\n",
    "        img_result.insert(0, img_con)\n",
    "        img_result.append(img_sty)\n",
    "        img_cat = torch.cat(img_result, dim=3)\n",
    "        img_cat = torch.unbind(img_cat, dim=0)\n",
    "        img_cat = torch.cat(img_cat, dim=1)\n",
    "        img_cat = dm.restore(img_cat.data.cpu())\n",
    "        output_img = torch.clamp(img_cat, 0, 1)\n",
    "\n",
    "        tt=transforms.ToPILImage()(output_img)\n",
    "        #tt.save('test_out/{}.png'.format(i))\n",
    "        gcf().set_size_inches(10,10)\n",
    "        imshow(tt)\n",
    "        show()\n",
    "        \n",
    "        if (i+1)%disp_step==0:\n",
    "            print('Testing {}/{} images'.format(i,len(dl)))\n",
    "\n",
    "\n",
    "gc_collected = gc.collect()\n",
    "gc.disable()\n",
    "\n",
    "print('Testing finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKurM8TPMqdA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AdaIN-answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
